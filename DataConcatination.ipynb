{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(columns=['Prompt', 'source', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "forbidden_question_set_df = pd.read_csv('data/forbidden_question_set_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "forbidden_question_set_df.drop(['Unnamed: 0.1', 'Unnamed: 0', 'Length', 'Perplexity', 'embedding', 'idx'], axis=1, inplace=True)\n",
    "forbidden_question_set_df['source'] = 'forbidden_questions'\n",
    "forbidden_question_set_df['target'] = '1'\n",
    "# forbidden_question_set_df['id'] = forbidden_question_set_df.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat([result_df, forbidden_question_set_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "jailbreak_prompts_df = pd.read_csv('data/jailbreak_prompts.csv')\n",
    "jailbreak_prompts_df.drop(['Unnamed: 0', 'idx', 'Length', 'Perplexity', 'embedding'], axis=1, inplace=True)\n",
    "jailbreak_prompts_df['source'] = 'jailbreak_prompts'\n",
    "jailbreak_prompts_df['target'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat([result_df, jailbreak_prompts_df])\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicous_deepset_df = pd.read_csv('data/malicous_deepset.csv')\n",
    "malicous_deepset_df.drop(['Unnamed: 0', 'idx', 'Length', 'Perplexity', 'embedding'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat([result_df, malicous_deepset_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionguard_df = pd.read_csv('data/predictionguard_df.csv')\n",
    "predictionguard_df.drop(['Unnamed: 0', 'idx', 'Length', 'Perplexity', 'embedding'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionguard_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat([result_df, predictionguard_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_train_promts = []\n",
    "with open('data/student_solutions.txt') as f:\n",
    "    given_train_promts = f.read().split('-----------------xyz-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for given_train_promts\n",
    "given_train_promts_df = pd.DataFrame(given_train_promts, columns=['Prompt'])\n",
    "given_train_promts_df['source'] = 'row_given_train'\n",
    "given_train_promts_df['target'] = '0'\n",
    "result_df = pd.concat([result_df, given_train_promts_df])\n",
    "\n",
    "# delete from memory all dataframes except result_df\n",
    "del forbidden_question_set_df, jailbreak_prompts_df, malicous_deepset_df, predictionguard_df, given_train_promts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leetcode_solutions_df = pd.read_csv('data/database_leetcode.csv')\n",
    "# leetcode_solutions_df.drop(['Unnamed: 0', \"Answer\"], axis=1, inplace=True)\n",
    "# leetcode_solutions_df.rename(columns={\"Question\": \"Prompt\"}, inplace=True)\n",
    "# leetcode_solutions_df['source'] = 'leetcode'\n",
    "# leetcode_solutions_df['target'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df = pd.concat([result_df, leetcode_solutions_df])\n",
    "# del leetcode_solutions_df\n",
    "# result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "leetcode_pythons = pd.read_parquet('data/leetcode_solutions.parquet')['python_solutions'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for leetcode_pythons\n",
    "leetcode_pythons_df = pd.DataFrame(leetcode_pythons, columns=['Prompt'])\n",
    "leetcode_pythons_df['source'] = 'leetcode'\n",
    "leetcode_pythons_df['target'] = '0'\n",
    "result_df = pd.concat([result_df, leetcode_pythons_df])\n",
    "\n",
    "del leetcode_pythons_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt_small_from_fug = pd.read_parquet('data/some_small_from_fug.parquete')\n",
    "smt_small_from_fug.rename(columns={\"text\": \"Prompt\", \"label\":\"target\"}, inplace=True)\n",
    "smt_small_from_fug['source'] ='smt_small_from_fug'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat([result_df, smt_small_from_fug])\n",
    "del smt_small_from_fug\n",
    "result_df.to_csv('data/result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the median length of prompts for each source\n",
    "result_df['prompt_length'] = result_df['Prompt'].str.len()\n",
    "result_df.groupby('source')['prompt_length'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def merge_prompts_as_comments(initial_text, injection_text):\n",
    "    # find all new lines in initial_text (separated by '\\n')\n",
    "    lines = initial_text.split('\\n')\n",
    "    # randomly choose a line to insert the injection text\n",
    "    line_to_insert = random.choice(lines)\n",
    "    # insert the injection text into the chosen line\n",
    "    new_text = initial_text.replace(line_to_insert, line_to_insert + '\\n' + \"# \" + injection_text)\n",
    "    return new_text\n",
    "\n",
    "def merge_prompts_as_containers(initial_text, injection_text):\n",
    "    # find all new lines in initial_text (separated by '\\n')\n",
    "    lines = initial_text.split('\\n')\n",
    "    # randomly choose a line to insert the injection text\n",
    "    line_to_insert = random.choice(lines)\n",
    "    # insert the injection text into the chosen line\n",
    "    new_text = initial_text.replace(line_to_insert, line_to_insert + '\\n' + \"string = \" + '\"'+injection_text+'\"')\n",
    "    return new_text\n",
    "\n",
    "def merge_prompts_concatenate(initial_text, injection_text):\n",
    "    # concatenate the two strings\n",
    "    new_text = initial_text + injection_text\n",
    "    return new_text\n",
    "def merge_prompts_first(initial_text, injection_text):\n",
    "    return injection_text + initial_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "infected_given_train = []\n",
    "injections = result_df[result_df['source'] == 'jailbreak_prompts']['Prompt'].to_list()\n",
    "for promt in given_train_promts:\n",
    "    injections = random.sample(injections, min(len(injections), 10))\n",
    "    infected_given_train.extend([merge_prompts_as_comments(promt, injection) for injection in injections])\n",
    "    infected_given_train.extend([merge_prompts_concatenate(promt, injection) for injection in injections])\n",
    "    infected_given_train.extend([merge_prompts_as_containers(promt, injection) for injection in injections])\n",
    "    infected_given_train.extend([merge_prompts_first(promt, injection) for injection in injections])\n",
    "    # cutted the lenght of injections\n",
    "    infected_given_train.extend([merge_prompts_as_comments(promt, '.'.join(injection.split('.')[0:2])) for injection in injections])\n",
    "    infected_given_train.extend([merge_prompts_concatenate(promt, '.'.join(injection.split('.')[0:2])) for injection in injections])\n",
    "    infected_given_train.extend([merge_prompts_as_containers(promt, '.'.join(injection.split('.')[0:2])) for injection in injections])\n",
    "    infected_given_train.extend([merge_prompts_first(promt, '.'.join(injection.split('.')[0:2])) for injection in injections])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the infected prompts to the result dataframe\n",
    "infected_given_train_df = pd.DataFrame(infected_given_train, columns=['Prompt'])\n",
    "infected_given_train_df['source'] = 'infected_given_train'\n",
    "infected_given_train_df['target'] = '1'\n",
    "result_df = pd.concat([result_df, infected_given_train_df])\n",
    "\n",
    "# save the result dataframe\n",
    "result_df.to_csv('data/result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows in each source category\n",
    "result_df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "short_df = pd.concat([\n",
    "    result_df[result_df['source'] == 'forbidden_questions'].sample(n=1000),\n",
    "    result_df[result_df['source'] == 'leetcode'].sample(n=2000),\n",
    "    result_df[result_df['source'] == 'infected_given_train'].sample(n=3000),\n",
    "    result_df[result_df['source'] == 'row_given_train'].sample(n=347),\n",
    "    result_df[result_df['source'] == 'jailbreak_prompts'].sample(n=2000)\n",
    "])\n",
    "short_df.to_csv('data/short_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
